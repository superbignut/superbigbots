##### 记录一下在功能实现的过程中，可能实现的idea

+ 二阶段、状态机似乎都是比较可行的交互框架

+ STDP的网络架构十分新奇，并且对于label的间接性依赖，说不定有助于"情感"的分类

    + 网路的输入层，其实可以使用前几个状态的指令、图像之类的，
    + 而当下时刻的指令就是一个一阶段的状态机
    + 情感模型的作用只是如果影响这个状态机的二阶段，比如，前进 + happy = 欢快的跳跃、之类的

    - 框架的缺点仍然是，好像是需要大量的训练过程！

+ 可以不断回想，也就是从回放池里面不断的抽出数据

+ 似乎用泊松编码不太行，想要突破的话，应该得使用脉冲编码 sstb 或者 mstb

+ 无监督分类似乎真实一个方法，指令动作 + 无监督输出情感+ 附加动作

+ 之所以用的无监督在图像分类中是可以用的是因为，同一个字母有类似的输入，进而可以进行分类，但是做rl类似的环境输入似乎不行

+ 那就借用rstdp的经验，把是否和人交互当作。 
    + 比如happy 和 交互 时 强化， sad 和不交互时，强化
    + emotion 和 交互是相反的时候，弱化 
    + 这里如果要做实验的话，可以把论文里的那个 方格的例子再实现一遍
    + 这里的话，把框架和这个同步实现吧
    + 两套网络在我这里似乎也可以实现，但是只对先发放的奖励可能不太能实现，应该还是要比较脉冲次数

+ stdp 的那个制约的概念看看能不能引入进来

+ 场景的话，比如一个人走得很快，走过来，就是一种输入，

    + 位置
    + 速度
    + 声音
    + 指令
    + 手势
    + 图像
    + 在真实世界中，还可以有时间的输入
    + 
    +  要多个时刻的输入吗-类似于highway_env

+ 输入没问题了，现在的问题是，怎么理解这些输入，把输入转化为对 emotion的判断、或者是输出，并进行反馈调节
    + 怎么实现的简单一点！
    + 先用纯判断来实现， 再逐步过渡到使用spaic
    + 还是先把spaic的rstdp 用起来

+ 奖励可以不是离散的，可以是一个类似于高斯分布的奖励函数，似乎更适配于rstdp


+ 训练的时候发现，脉冲的输出有的时候会全部是0，那么这样的话，其实可以做两个网络，回到rstdp的那个思路上，
+ 比如是 car-pole 问题就是 左推和右推 各占一个网络，如果
+ 达到一定的评标标准，就同时正向训练两个网络
+ 否则同时弱化两个
+ 但如果两个都没释放的话，强化正确的那个？？？，在狗上似乎可以强化 ，但是cp上，不太行吧，
+ 这个问题的奖励还是不太好设定欸， 不只是cp 上，也是 狗上， 我怎么评价一个episode 或者 一个时刻的动作的好坏呢,不不不，car-pole上其实也是根据观测来得到的奖励
    + 比如先做一个null 的动作，得到一个奖励，再去做一次网络的前向传播
+ 这连个问题似乎有点 相似性


+ 两个网络的话，用ann做吗，用ann的话我又不想用反向传播，好像权重更新这里不太行欸，ann的话就是单纯的+1 或者 -1 的奖励吗？
+ ann 的话无法做到，简单的强化之前的输出这种效果、
+ 那好如果就是使用snn， 那我要在哪个例子中试一下效果呢


+ rstdp 应该是用不了，只能用stdp来做了 stdp 这个能不能魔改一下， 或者有的stdp 有的层 stca # 不不不不不不不 这里 reward 如果传入0 的话，就相当于没有更新，可以先看传入0 看一下输出，再传入别的

+ 所以现在是要用 rstdp 就要reward 先传进去
+ 或者不同spaic的rstdp

+ 或者就像杨老师说的那样，只是做个简单的不需要大规模训练的SNN自适应出来

+ 因为spaic的奖励要在最开始，得到观测的时候就给出，因此是不是可以给出一个奖励网络，当然也可以进行学习，然后写死一个奖励惩罚机制，并对实际情况进行微调

+ 奖励模型从状态中得到奖励   这里相当于一种浅层次的特征提取 概率输出-软标签 对各个类别的信息， 简单的就是一个函数，复杂的说不定也可以用RL来做

+ 使用奖励进行 rstdp 的 SNN 网络训练  进一步的特征提取

+ SNN的输出是一个情感信号，影响交互动作

+ 交互结果调节奖励模型（自适应）

+ 二阶段是主要框架

+ 先用 rstdp 做一个手写体的2分类试一下 0-4 和 5-9

+ 把stdp 魔改了， 现在也可以传入 reward

+ 发现，stdp_example 中的例子 由于最后的那种统计特性，现在我的感觉就是，似乎可以把每次真实的交互 根据是否交互成功，进而把数据库放到 合适的logel下面

+ 最开始的数据 可以手动构造，进而得到一个还算比较合理的权重分布，然后对实际情况进行微调

+ 如果可行的话，再把 reward 用上， 对  重要的例子，加大更新力度

+ 这里还有一个比较相似的地方是，比如说 stdp以前的输入都是 0 和 1，结果突然来了一个 5 ，如果这这个交互很重要的话，需要想办法可以快速把权重 修改掉

+ 现在看来的话，暂时不用奖励器，反而是预训练比较重要，让机器狗，现有一个比较正常的输出，再对真实情况进行调整， 并根据真实数据不断排除掉初始数据

+ 这里在minist 中的体现就是，最开始是01 的数据进行的训练，但是当我有5 过来的时候，我如果认为5归属于"1", 那么就要有old的1被去除， 新的 5 更新数据

+ 这里的投票规则感觉还可以改改，比如每个神经元有两个归属，应该很自然的就可以提高准确率

+ 这里其实是叫knn 或者 kmeans 的类似于投票的归类方式

+ Todo  看一下，论文中还有提到的 其他无监督学习方法还有 引入多层的可能性

+ 如果是数据输入的话，和图像输入不同，即使归一化了，也是不规则的输入，似乎 在后面的网络中也可以引入一些不规则的性，来抵消这一影响

+ stdp 会导致 emotion 输出的不稳定性，也需要考虑